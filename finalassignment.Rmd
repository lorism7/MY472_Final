---
title: "MY472 Final Assignment"
output: html_document
date: "2024-01-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

My public repository for this assignment can be found [here](https://github.com/lorism7/MY472_Final.git).

### Introduction

This analysis examines potential biases in the UK's police stop and search practices, focusing on officer-perceived ethnicity. We sourced and preprocessed data from the UK Police database and the Office for National Statistics, concentrating on demographic and geographic factors. Our key metric is the proportion of false stop and searches by ethnicity and region, a potential indicator of biased policing. Our methodology involves visualization and statistical analysis to understand these law enforcement practices and their implications for fairness and public trust.

### Data

#### UK Police Data API

We gathered 2021 stop and search data from the UK Police API, by police force, for detailed regional insights. This approach was chosen for its precision in capturing localized policing patterns and ensuring a comprehensive view across different regions. Each police force's data was collected monthly, ensuring granularity in the time sequence and allowing for the detection of temporal trends.

The year 2021 was specifically selected to balance the depth of data with practicality. It offers a recent snapshot, reflecting contemporary policing practices and societal contexts, making our analysis both relevant and timely.

During data processing, we ensured consistency and uniformity in the datasets. This involved standardizing the data structure across different records, removing missing data, and merging the datasets to form a single, cohesive database. Key variables included age, gender, officer-defined ethnicity, and location.

Another critical aspect of our analysis involved the creation of a new variable to identify false stop and searches. This was defined as instances where the outcome of a stop and search was not linked to the object of the search (FALSE). By calculating the proportion of false stop and searches for each ethnicity, we could gain insights into potential biases in policing practices.

For efficiency and reproducibility, we saved the processed data as a CSV file. The "ne_10m_admin_0_countries" shapefile was used for spatial analysis.

#### Additional Source

To handle and clean the demographic data obtained from the UK Office for National Statistics, a methodical approach was applied. The cleaned data was then enriched by calculating the total percentage of non-white ethnic groups in each area. Finally, we merged with a shapefile, titled "Local Authority Districts (May 2021) UK BGC and containing geographic boundaries of UK local authority districts as of May 2021, which offered a precise and contextually relevant geographical framework for the study.


```{r getting data from the uk police api}
# THIS CHUNK OF CODE PERFORMS DATA RETRIVAL FROM THE UK POLICE API. IT HAS BEEN RUN ONCE AND SAVED IN THE "police_stops_data.csv" TO REDUCE RUNTIME AND INCREASE EFFICIENCY

# loading the necessary libraries
library(httr)
library(jsonlite)
library(lubridate)

# forces <- c("avon-and-somerset", "bedfordshire", "btp", "cambridgeshire", "cheshire", 
#            "city-of-london", "cleveland", "cumbria", "derbyshire", "devon-and-cornwall", 
#            "dorset", "durham", "dyfed-powys", "essex", "gloucestershire", "gmp", "gwent", 
#            "hampshire", "hertfordshire", "humberside", "kent", "lancashire", "leicestershire", 
#            "lincolnshire", "merseyside", "metropolitan", "norfolk", "north-wales", 
#            "north-yorkshire", "northamptonshire", "northumbria", "nottinghamshire", "psni", 
#            "south-wales", "south-yorkshire", "staffordshire", "suffolk", "surrey", 
#            "sussex", "thames-valley", "warwickshire", "west-mercia", "west-midlands", 
#            "west-yorkshire", "wiltshire")

# date_seq <- c("2021-01", "2021-02", "2021-03", "2021-04", "2021-05",
#                "2021-06", "2021-07", "2021-08", "2021-09", "2021-10", "2021-11", "2021-12")


# Define the columns to keep
# columns_to_keep <- c("age_range", "gender", "datetime", "officer_defined_ethnicity", 
               #      "type", "outcome_linked_to_object_of_search", "location.latitude", 
               #      "location.longitude", "location.street.id", "location.street.name", "force")


#all_data <- list()

#for (force in forces) {
#    force_data <- list()

#    for (date in date_seq) {
#        url <- paste0("https://data.police.uk/api/stops-force?force=", force, "&date=", date)
#        response <- GET(url)
#        if (status_code(response) == 200) {
#            data <- fromJSON(content(response, "text", encoding = "UTF-8"), flatten = TRUE)
#            if (is.data.frame(data) && nrow(data) > 0) {
#                data$force <- force  # Add the force name here

#                 Standardize columns
#                for (col in columns_to_keep) {
#                    if (!col %in% names(data)) {
#                        data[[col]] <- NA  # Add missing columns as NA
#                    }
#                }
#                data <- data[, columns_to_keep]  # Keep only the desired columns

#                force_data[[date]] <- data
#            } else {
#                warning("No data or incorrect format for ", force, " for date ", date)
#            }
#        } else {
#            warning("Failed to retrieve data for ", force, " for date ", date)
#        }
#    }

#     Combine data frames for this force
#    if (length(force_data) > 0) {
#        all_data[[force]] <- do.call(rbind, force_data)
#    }
#}

# Combine all the data into one data frame
#combined_data <- do.call(rbind, all_data)

# Handle Missing Values
#combined_data <- na.omit(combined_data)

# Convert 'datetime' to Date format
#combined_data$datetime <- as.Date(combined_data$datetime, format="%Y-%m-%dT%H:%M:%S") 

#head(combined_data)

# Save the combined data to a CSV file
#write.csv(combined_data, "police_stops_data.csv", row.names = FALSE)

# Read the data from the saved file
combined_data <- read.csv("police_stops_data.csv")

```

```{r percentage of non-white}
# Load necessary libraries
library(sf)
library(dplyr)
library(ggplot2)
library(readr)
library(viridis)


# Read the shapefile
lad_shapefile <- st_read("Local_Authority_Districts_(May_2021)_UK_BGC/LAD_MAY_2021_UK_BGC.shp", quiet = TRUE)

# Read the CSV data
data <- read_csv2("ethnicity-composition-across-uk.csv")


# sum up all non-white ethnicity percentages.
library(dplyr)


clean_data <- data %>% 
  filter(`Asian, Asian British or Asian Welsh: Bangladesh` != "-", 
                           `Asian, Asian British or Asian Welsh: Chinese` != "-",
                           `Asian, Asian British or Asian Welsh: Indian` != "-",
                           `Asian, Asian British or Asian Welsh: Pakistani`!= "-",
                           `Asian, Asian British or Asian Welsh: Other Asian` != "-",
                           `Black, Black British, Black Welsh, Caribbean or African: African` != "-",
                           `Black, Black British, Black Welsh, Caribbean or African: Caribbean` != "-",
                           `Black, Black British, Black Welsh, Caribbean or African: Other Black` != "-", 
                           `Mixed or Multiple ethnic groups: White and Asian` != "-", 
                           `Mixed or Multiple ethnic groups: White and Black African` != "-", 
                           `Mixed or Multiple ethnic groups: White and Black Caribbean` != "-",
                           `Mixed or Multiple ethnic groups: Other Mixed or Multiple ethnic groups` != "-",
                           `Other ethnic group: Arab` != "-",
                           `Other ethnic group: Any other ethnic group` != "-")

data <- clean_data %>%
  mutate(
    `Asian, Asian British or Asian Welsh: Bangladesh` = as.numeric(gsub(",", ".", `Asian, Asian British or Asian Welsh: Bangladesh`)),
    `Asian, Asian British or Asian Welsh: Chinese` = as.numeric(gsub(",", ".", `Asian, Asian British or Asian Welsh: Chinese`)),
    `Asian, Asian British or Asian Welsh: Indian` = as.numeric(gsub(",", ".", `Asian, Asian British or Asian Welsh: Indian`)),
    `Asian, Asian British or Asian Welsh: Pakistani` = as.numeric(gsub(",", ".", `Asian, Asian British or Asian Welsh: Pakistani`)),
    `Asian, Asian British or Asian Welsh: Other Asian` = as.numeric(gsub(",", ".", `Asian, Asian British or Asian Welsh: Other Asian`)),
    `Black, Black British, Black Welsh, Caribbean or African: African` = as.numeric(gsub(",", ".", `Black, Black British, Black Welsh, Caribbean or African: African`)),
    `Black, Black British, Black Welsh, Caribbean or African: Caribbean` = as.numeric(gsub(",", ".", `Black, Black British, Black Welsh, Caribbean or African: Caribbean`)),
    `Black, Black British, Black Welsh, Caribbean or African: Other Black` = as.numeric(gsub(",", ".", `Black, Black British, Black Welsh, Caribbean or African: Other Black`)),
    `Mixed or Multiple ethnic groups: White and Asian` = as.numeric(gsub(",", ".", `Mixed or Multiple ethnic groups: White and Asian`)),
    `Mixed or Multiple ethnic groups: White and Black African` = as.numeric(gsub(",", ".", `Mixed or Multiple ethnic groups: White and Black African`)),
    `Mixed or Multiple ethnic groups: White and Black Caribbean` = as.numeric(gsub(",", ".", `Mixed or Multiple ethnic groups: White and Black Caribbean`)),
    `Mixed or Multiple ethnic groups: Other Mixed or Multiple ethnic groups` = as.numeric(gsub(",", ".", `Mixed or Multiple ethnic groups: Other Mixed or Multiple ethnic groups`)),
    `Other ethnic group: Arab` = as.numeric(gsub(",", ".", `Other ethnic group: Arab`)),
    `Other ethnic group: Any other ethnic group` = as.numeric(gsub(",", ".", `Other ethnic group: Any other ethnic group`))
  )


data <- data %>%
  rowwise() %>%
  mutate(PercentNonWhite = `Asian, Asian British or Asian Welsh: Bangladesh` +
                           `Asian, Asian British or Asian Welsh: Chinese` +
                           `Asian, Asian British or Asian Welsh: Indian` +
                           `Asian, Asian British or Asian Welsh: Pakistani` +
                           `Asian, Asian British or Asian Welsh: Other Asian` +
                           `Black, Black British, Black Welsh, Caribbean or African: African` +
                           `Black, Black British, Black Welsh, Caribbean or African: Caribbean` +
                           `Black, Black British, Black Welsh, Caribbean or African: Other Black` +
                           `Mixed or Multiple ethnic groups: White and Asian` +
                           `Mixed or Multiple ethnic groups: White and Black African` +
                           `Mixed or Multiple ethnic groups: White and Black Caribbean` +
                           `Mixed or Multiple ethnic groups: Other Mixed or Multiple ethnic groups` +
                           `Other ethnic group: Arab` +
                           `Other ethnic group: Any other ethnic group`) %>%
  ungroup()




# Merge the shapefile and CSV data
map_data <- left_join(lad_shapefile, data, by = c("LAD21CD" = "Area code"))

# Assuming your merged shapefile and data is in 'uk_data'
map1 <- ggplot(data = map_data) +
  geom_sf(aes(fill = PercentNonWhite), color = NA) +  # No border color
  scale_fill_viridis_c(
    option = "C",  # Viridis color option
    direction = 1,  # Sets the direction of the color scale
    name = "Percent Non-White",  # Legend title
    labels = scales::percent_format(scale = 1),  # Format labels as percentages
    na.value = "grey"  # Color for NA values
  ) +
  labs(
    title = "Percentage of Non-White Ethnicities",
    caption = "Source: data.gov.uk"
  ) +
  theme_minimal() +
  theme(
    legend.position = "right",  # Adjust legend position
    plot.title = element_text(size = 14, face = "bold"),  # Title style
    plot.subtitle = element_text(size = 12)  # Subtitle style
  )

```


```{r stop-and-search incidents distribution}
library(sf)
library(dplyr)
library(ggplot2)

# Assuming the shapefiles are in the specified directory
uk_shape <- st_read('ne_10m_admin_0_countries/ne_10m_admin_0_countries.shp', quiet = TRUE)

# Filter for the United Kingdom
uk <- uk_shape %>% 
  filter(ADMIN == "United Kingdom")

combined_data$location.latitude <- as.numeric(as.character(combined_data$location.latitude))
combined_data$location.longitude <- as.numeric(as.character(combined_data$location.longitude))

# Create a simple feature object from your data
stops_sf <- st_as_sf(combined_data, coords = c("location.longitude", "location.latitude"), 
                     crs = st_crs(uk), agr = "constant")


map2 <- ggplot() +
  geom_sf(data = uk) +  # Plot the UK map
  geom_sf(data = stops_sf, color = 'red', size = 0.6, alpha = 0.7) +  # Plot the stop and search data
  theme_minimal() +
  labs(title = "Distribution of Stop and Searches",
    caption = "Source: data.police.uk"
  ) +
  theme(
    plot.title = element_text(size = 14, face = "bold") # Title style
  )

```
### Analysis

##### Figure 1: Ethnic Demographics and Police Stop-and-Searches in the UK
```{r map together in a single visualisation}
library(ggplot2)

# Modify map1 to remove axes
map1 <- map1 + 
  coord_sf() +  # Add coordinate system for spatial data+ 
  theme(
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank(),
    plot.title = element_text(hjust = 0.5, size = 10),  # Adjust title size
    axis.title = element_blank(),  # Remove axis titles
    axis.text = element_blank(),   # Remove axis text
    axis.ticks = element_blank(),   # Remove axis ticks
    legend.position = "left", # Move legend to bottom
    legend.title = element_text(size = 7),  # Adjust the size of the legend title
    legend.text = element_text(size = 7),    # Adjust the size of the legend text
    legend.key.size = unit(0.75, "lines") # Adjust the size of the legend keys
  )


# Modify map2 to remove axes
map2 <- map2 + 
  coord_sf() +  # Add coordinate system for spatial data+ 
  theme(
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank(),
    plot.title = element_text(hjust = 0.5, size = 10),  # Adjust title size
    axis.title = element_blank(),  # Remove axis titles
    axis.text = element_blank(),   # Remove axis text
    axis.ticks = element_blank()   # Remove axis ticks
  )

# Adjust margins of the maps
map1 <- map1 + theme(plot.margin = margin(1, 1, 1, 1, "cm"))
map2 <- map2 + theme(plot.margin = margin(1, 1, 1, 1, "cm"))

# Combine the plots with patchwork
library(patchwork)
combined_map_layout <- map1 + map2 +
  plot_layout(ncol = 2)

# Print the combined plot
print(combined_map_layout)
```

Figure 1 contrasts two maps of the UK, one depicting the density of non-white ethnic groups with London as the focal point, and the other indicating stop-and-search incidents, similarly clustered around London. This visual pairing hints at a potential link between ethnic composition and the frequency of police interventions, with both phenomena most pronounced in the capital. However, also taking into account the missing data (grey areas), we need deeper examination to discern any underlying causality, as the maps alone merely suggest a spatial alignment without establishing a direct connection.


##### Figure 2
```{r bar chart}
# Load necessary libraries
library(tidyverse)

# Assuming your data is loaded into a dataframe named 'police_stops_data'

# Read the CSV file (adjust the path as needed)
police_stops_data <- read.csv('police_stops_data.csv')

# Creating a new variable for false stop and searches
# A false stop and search is one where the outcome is not linked to the object of search
police_stops_data$false_stop_search <- !police_stops_data$outcome_linked_to_object_of_search

# Calculate the proportion of false stop and searches for each ethnicity
ethnicity_summary <- police_stops_data %>%
  group_by(officer_defined_ethnicity) %>%
  summarise(
    total_searches = n(),
    false_searches = sum(false_stop_search),
    proportion_false_searches = false_searches / total_searches
  )

# Plotting the proportion of false stop and searches by ethnicity
ggplot(ethnicity_summary, aes(x = officer_defined_ethnicity, y = proportion_false_searches, fill = officer_defined_ethnicity)) +
  geom_bar(stat = 'identity') +
  scale_fill_brewer(palette = "Set2") +
  geom_text(aes(label = round(proportion_false_searches, 3)), vjust = -0.5) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12),
    legend.position = "none"
  ) +
  labs(
    title = 'Proportion of False Stop and Searches by Ethnicity',
    subtitle = 'Comparison across different ethnic groups',
    x = 'Officer-Defined Ethnicity',
    y = 'Proportion of False Searches'
  )

```

Figure 2 presents a clear visual representation of the proportion of false stops and searches across different ethnic groups, showing a marked variance that could indicate an ethnic bias. Notably, individuals identified as Black and those grouped as 'Other' encounter the highest rates of false stops and searches, significantly more than their Asian counterparts, who experience the lowest. This disparity calls for a critical evaluation of policing tactics to address prejudiced assumptions rooted in perceived ethnicity.


##### Figure 3
```{r adding police force to the analysis}
# Load necessary libraries
library(tidyverse)

# Assuming your data is loaded into a dataframe named 'police_stops_data'

# Read the CSV file (adjust the path as needed)
police_stops_data <- read.csv('police_stops_data.csv')

# Creating a new variable for false stop and searches
police_stops_data$false_stop_search <- !police_stops_data$outcome_linked_to_object_of_search

# Group by region (force) and ethnicity, and calculate proportions
regional_ethnicity_summary <- police_stops_data %>%
  group_by(force, officer_defined_ethnicity) %>%
  summarise(
    total_searches = n(),
    false_searches = sum(false_stop_search),
    proportion_false_searches = false_searches / total_searches
  ) %>%
  ungroup()

ggplot(regional_ethnicity_summary, aes(x = force, y = proportion_false_searches, color = officer_defined_ethnicity, group = officer_defined_ethnicity)) +
  geom_line() +
  geom_point() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(
    title = 'Trends in False Stop and Searches by Ethnicity Across Regions',
    subtitle = 'Each line represents an ethnicity',
    x = 'Region (Police Force)',
    y = 'Proportion of False Searches',
    color = 'Ethnicity'
  )

```

Figure 3 depicts the inconsistency of false stop and searches across regions for different ethnic groups, suggesting that the application of stop and search is not uniform. The fluctuation might reflect regional policing strategies, community relations, or social-economic factors influencing police interactions. The peaks and troughs for each ethnic line warrant a closer examination of local policing policies to understand the underlying causes of these disparities.


##### Results of the Regression Analysis (rows 1-6)
```{r regression}
# Load necessary libraries
library(dplyr)

# Read your data into a dataframe
# Read the CSV file into a dataframe
data <- read.csv('police_stops_data.csv')


# Define false stop and searches
# Assuming a false stop and search is when 'outcome_linked_to_object_of_search' is FALSE
data$false_stop_search <- !data$outcome_linked_to_object_of_search


# Convert categorical variables to factors
data$force <- as.factor(data$force)
data$officer_defined_ethnicity <- as.factor(data$officer_defined_ethnicity)
data$false_stop_search <- as.factor(data$outcome_linked_to_object_of_search)

# Fit a logistic regression model
logistic_model <- glm(false_stop_search ~ force + officer_defined_ethnicity, data = data, family = 'binomial')

library(broom)

# Assuming your logistic model is named logistic_model
tidied_model <- tidy(logistic_model)

# Selecting important columns
important_results <- tidied_model %>%
  select(term, estimate, std.error, statistic, p.value)

# Viewing the table
head(important_results)


```

The regression analysis quantitatively affirms that disparities in stop and search practices are not merely random fluctuations but are instead significantly influenced by regional policing policies and potentially by the intersectional dynamics of ethnicity within these locales. This  compels a targeted investigation into the fabric of regional policy frameworks.


### Limitations

Our analysis, while revealing, has limitations. The 2021 data provides a recent snapshot but may not capture longer-term trends. The absence of time-of-day data limits our ability to explore temporal patterns in police behavior. Some forces only record dates, not specific times, resulting in a skewed distribution of incidents around midnight.  Moreover, the complexity of societal factors influencing police actions are not fully captured. Future research could benefit from a multi-dimensional approach, integrating additional variables and longitudinal data to provide a more nuanced understanding of policing biases.



## Appendix: All code in this assignment

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE} 
# this chunk generates the complete code appendix. 
# eval=FALSE tells R not to run (``evaluate'') the code here (it was already run before).
```
