---
title: "MY472 Final Assignment"
output: html_document
date: "2024-01-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

My public repository for this assignment can be found [here](https://github.com/lorism7/MY472_Final.git).

### Introduction

This analysis seeks to identify potential biases in the UK's police stop and search practices, particularly regarding officer-perceived ethnicity. Our methodology involves data sourcing and preprocessing from the UK Police database and the Office for National Statistics, focusing on demographic and geographic aspects. We place significant emphasis on calculating and analyzing the proportion of false stop and searches by ethnicity and region, as this metric could reflect the accuracy and justification of police actions, indicating potential discriminatory biases if certain ethnic groups are disproportionately subject to unjustified searches. Through both visualization and statistical analysis, our approach aims to provide a comprehensive understanding of these law enforcement practices and their implications for fairness and public trust.

### Data

#### UK Police Data API

For the study's primary data source, we meticulously gathered stop and search data from the UK Police API, tailored by individual police forces and confined to the year 2021. This approach was chosen for its precision in capturing localized policing patterns and ensuring a comprehensive view across different regions. Each police force's data was collected monthly, ensuring granularity in the time sequence and allowing for the detection of temporal trends.

The year 2021 was specifically selected to balance the depth of data with practicality. It offers a recent snapshot, reflecting contemporary policing practices and societal contexts, making our analysis both relevant and timely.

During data processing, we ensured consistency and uniformity in the datasets. This involved standardizing the data structure across different records, removing missing data, and merging the datasets to form a single, cohesive database. The data included key variables such as age, gender, officer-defined ethnicity, and geographic coordinates, providing a rich basis for our analysis.

Another critical aspect of our analysis involved the creation of a new variable to identify false stop and searches. This was defined as instances where the outcome of a stop and search was not linked to the object of the search (FALSE). By calculating the proportion of false stop and searches for each ethnicity, we could gain insights into potential biases in policing practices.

Due to the extensive size of the data and to optimize the run-time of our analysis, we saved the processed data as a CSV file. This step not only streamlined our workflow but also ensured that our analysis was reproducible and efficient. For spatial analysis,  we utilized the "ne_10m_admin_0_countries" shapefile as our geographic data source.

#### Additional Source

To handle and clean the demographic data obtained from the UK Office for National Statistics, a methodical approach was applied. After being filtered and transformed, the cleaned data was  enriched by calculating the total percentage of non-white ethnic groups in each area. This was achieved by summing the percentages of all non-white ethnic groups row-wise, providing a comprehensive measure of ethnic diversity across different regions.

Finally, this processed demographic data was merged with a shapefile, enabling spatial analysis and visualization of ethnic compositions in correlation with police stop and search data. The shapefile, titled "Local Authority Districts (May 2021) UK BGC and containing geographic boundaries of UK local authority districts as of May 2021, offered a precise and contextually relevant geographical framework for the study.



```{r getting data from the uk police api}
library(httr)
library(jsonlite)
library(lubridate)

# forces <- c("avon-and-somerset", "bedfordshire", "btp", "cambridgeshire", "cheshire", 
#            "city-of-london", "cleveland", "cumbria", "derbyshire", "devon-and-cornwall", 
 #           "dorset", "durham", "dyfed-powys", "essex", "gloucestershire", "gmp", "gwent", 
  #          "hampshire", "hertfordshire", "humberside", "kent", "lancashire", "leicestershire", 
   #         "lincolnshire", "merseyside", "metropolitan", "norfolk", "north-wales", 
    #        "north-yorkshire", "northamptonshire", "northumbria", "nottinghamshire", "psni", 
     #       "south-wales", "south-yorkshire", "staffordshire", "suffolk", "surrey", 
     #       "sussex", "thames-valley", "warwickshire", "west-mercia", "west-midlands", 
      #      "west-yorkshire", "wiltshire")

# date_seq <- c("2021-01", "2021-02", "2021-03", "2021-04", "2021-05",
#                "2021-06", "2021-07", "2021-08", "2021-09", "2021-10", "2021-11", "2021-12")


# Define the columns to keep
# columns_to_keep <- c("age_range", "gender", "datetime", "officer_defined_ethnicity", 
               #      "type", "outcome_linked_to_object_of_search", "location.latitude", 
               #      "location.longitude", "location.street.id", "location.street.name", "force")


#all_data <- list()

#for (force in forces) {
  #  force_data <- list()

    #for (date in date_seq) {
        #url <- paste0("https://data.police.uk/api/stops-force?force=", force, "&date=", date)
        #response <- GET(url)
       # if (status_code(response) == 200) {
     #       data <- fromJSON(content(response, "text", encoding = "UTF-8"), flatten = TRUE)
       #     if (is.data.frame(data) && nrow(data) > 0) {
      #          data$force <- force  # Add the force name here

                # Standardize columns
        #        for (col in columns_to_keep) {
         #           if (!col %in% names(data)) {
          #              data[[col]] <- NA  # Add missing columns as NA
           #         }
            #    }
             #   data <- data[, columns_to_keep]  # Keep only the desired columns

              #  force_data[[date]] <- data
          #  } else {
           #     warning("No data or incorrect format for ", force, " for date ", date)
            #}
      #  } else {
     #       warning("Failed to retrieve data for ", force, " for date ", date)
      #  }
  #  }

    # Combine data frames for this force
   # if (length(force_data) > 0) {
#        all_data[[force]] <- do.call(rbind, force_data)
 #   }
#}

# Combine all the data into one data frame
#combined_data <- do.call(rbind, all_data)

# Handle Missing Values
#combined_data <- na.omit(combined_data)

# Convert 'datetime' to Date format
#combined_data$datetime <- as.Date(combined_data$datetime, format="%Y-%m-%dT%H:%M:%S") 

#head(combined_data)

# Save the combined data to a CSV file
#write.csv(combined_data, "police_stops_data.csv", row.names = FALSE)

# Read the data from the saved file
combined_data <- read.csv("police_stops_data.csv")

```

```{r percentage of non-white}
# Load necessary libraries
library(sf)
library(dplyr)
library(ggplot2)
library(readr)
library(viridis)


# Read the shapefile
lad_shapefile <- st_read("Local_Authority_Districts_(May_2021)_UK_BGC/LAD_MAY_2021_UK_BGC.shp", quiet = TRUE)

# Read the CSV data
data <- read_csv2("ethnicity-composition-across-uk.csv")


# sum up all non-white ethnicity percentages.
library(dplyr)


clean_data <- data %>% 
  filter(`Asian, Asian British or Asian Welsh: Bangladesh` != "-", 
                           `Asian, Asian British or Asian Welsh: Chinese` != "-",
                           `Asian, Asian British or Asian Welsh: Indian` != "-",
                           `Asian, Asian British or Asian Welsh: Pakistani`!= "-",
                           `Asian, Asian British or Asian Welsh: Other Asian` != "-",
                           `Black, Black British, Black Welsh, Caribbean or African: African` != "-",
                           `Black, Black British, Black Welsh, Caribbean or African: Caribbean` != "-",
                           `Black, Black British, Black Welsh, Caribbean or African: Other Black` != "-", 
                           `Mixed or Multiple ethnic groups: White and Asian` != "-", 
                           `Mixed or Multiple ethnic groups: White and Black African` != "-", 
                           `Mixed or Multiple ethnic groups: White and Black Caribbean` != "-",
                           `Mixed or Multiple ethnic groups: Other Mixed or Multiple ethnic groups` != "-",
                           `Other ethnic group: Arab` != "-",
                           `Other ethnic group: Any other ethnic group` != "-")

data <- clean_data %>%
  mutate(
    `Asian, Asian British or Asian Welsh: Bangladesh` = as.numeric(gsub(",", ".", `Asian, Asian British or Asian Welsh: Bangladesh`)),
    `Asian, Asian British or Asian Welsh: Chinese` = as.numeric(gsub(",", ".", `Asian, Asian British or Asian Welsh: Chinese`)),
    `Asian, Asian British or Asian Welsh: Indian` = as.numeric(gsub(",", ".", `Asian, Asian British or Asian Welsh: Indian`)),
    `Asian, Asian British or Asian Welsh: Pakistani` = as.numeric(gsub(",", ".", `Asian, Asian British or Asian Welsh: Pakistani`)),
    `Asian, Asian British or Asian Welsh: Other Asian` = as.numeric(gsub(",", ".", `Asian, Asian British or Asian Welsh: Other Asian`)),
    `Black, Black British, Black Welsh, Caribbean or African: African` = as.numeric(gsub(",", ".", `Black, Black British, Black Welsh, Caribbean or African: African`)),
    `Black, Black British, Black Welsh, Caribbean or African: Caribbean` = as.numeric(gsub(",", ".", `Black, Black British, Black Welsh, Caribbean or African: Caribbean`)),
    `Black, Black British, Black Welsh, Caribbean or African: Other Black` = as.numeric(gsub(",", ".", `Black, Black British, Black Welsh, Caribbean or African: Other Black`)),
    `Mixed or Multiple ethnic groups: White and Asian` = as.numeric(gsub(",", ".", `Mixed or Multiple ethnic groups: White and Asian`)),
    `Mixed or Multiple ethnic groups: White and Black African` = as.numeric(gsub(",", ".", `Mixed or Multiple ethnic groups: White and Black African`)),
    `Mixed or Multiple ethnic groups: White and Black Caribbean` = as.numeric(gsub(",", ".", `Mixed or Multiple ethnic groups: White and Black Caribbean`)),
    `Mixed or Multiple ethnic groups: Other Mixed or Multiple ethnic groups` = as.numeric(gsub(",", ".", `Mixed or Multiple ethnic groups: Other Mixed or Multiple ethnic groups`)),
    `Other ethnic group: Arab` = as.numeric(gsub(",", ".", `Other ethnic group: Arab`)),
    `Other ethnic group: Any other ethnic group` = as.numeric(gsub(",", ".", `Other ethnic group: Any other ethnic group`))
  )


data <- data %>%
  rowwise() %>%
  mutate(PercentNonWhite = `Asian, Asian British or Asian Welsh: Bangladesh` +
                           `Asian, Asian British or Asian Welsh: Chinese` +
                           `Asian, Asian British or Asian Welsh: Indian` +
                           `Asian, Asian British or Asian Welsh: Pakistani` +
                           `Asian, Asian British or Asian Welsh: Other Asian` +
                           `Black, Black British, Black Welsh, Caribbean or African: African` +
                           `Black, Black British, Black Welsh, Caribbean or African: Caribbean` +
                           `Black, Black British, Black Welsh, Caribbean or African: Other Black` +
                           `Mixed or Multiple ethnic groups: White and Asian` +
                           `Mixed or Multiple ethnic groups: White and Black African` +
                           `Mixed or Multiple ethnic groups: White and Black Caribbean` +
                           `Mixed or Multiple ethnic groups: Other Mixed or Multiple ethnic groups` +
                           `Other ethnic group: Arab` +
                           `Other ethnic group: Any other ethnic group`) %>%
  ungroup()




# Merge the shapefile and CSV data
map_data <- left_join(lad_shapefile, data, by = c("LAD21CD" = "Area code"))

# Assuming your merged shapefile and data is in 'uk_data'
map1 <- ggplot(data = map_data) +
  geom_sf(aes(fill = PercentNonWhite), color = NA) +  # No border color
  scale_fill_viridis_c(
    option = "C",  # Viridis color option
    direction = 1,  # Sets the direction of the color scale
    name = "Percent Non-White",  # Legend title
    labels = scales::percent_format(scale = 1),  # Format labels as percentages
    na.value = "grey"  # Color for NA values
  ) +
  labs(
    title = "Percentage of Non-White Ethnicities",
    caption = "Source: data.gov.uk"
  ) +
  theme_minimal() +
  theme(
    legend.position = "right",  # Adjust legend position
    plot.title = element_text(size = 14, face = "bold"),  # Title style
    plot.subtitle = element_text(size = 12)  # Subtitle style
  )

```


```{r stop-and-search incidents distribution}
library(sf)
library(dplyr)
library(ggplot2)

# Assuming the shapefiles are in the specified directory
uk_shape <- st_read('ne_10m_admin_0_countries/ne_10m_admin_0_countries.shp', quiet = TRUE)

# Filter for the United Kingdom
uk <- uk_shape %>% 
  filter(ADMIN == "United Kingdom")

combined_data$location.latitude <- as.numeric(as.character(combined_data$location.latitude))
combined_data$location.longitude <- as.numeric(as.character(combined_data$location.longitude))

# Create a simple feature object from your data
stops_sf <- st_as_sf(combined_data, coords = c("location.longitude", "location.latitude"), 
                     crs = st_crs(uk), agr = "constant")


map2 <- ggplot() +
  geom_sf(data = uk) +  # Plot the UK map
  geom_sf(data = stops_sf, color = 'red', size = 0.6, alpha = 0.7) +  # Plot the stop and search data
  theme_minimal() +
  labs(title = "Distribution of Stop and Searches",
    caption = "Source: data.police.uk"
  ) +
  theme(
    plot.title = element_text(size = 14, face = "bold") # Title style
  )

```
### Analysis

##### Figure 1: Ethnic Demographics and Police Stop-and-Searches in the UK
```{r map together in a single visualisation}
library(ggplot2)

# Modify map1 to remove axes
map1 <- map1 + 
  coord_sf() +  # Add coordinate system for spatial data+ 
  theme(
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank(),
    plot.title = element_text(hjust = 0.5, size = 10),  # Adjust title size
    axis.title = element_blank(),  # Remove axis titles
    axis.text = element_blank(),   # Remove axis text
    axis.ticks = element_blank(),   # Remove axis ticks
    legend.position = "left", # Move legend to bottom
    legend.title = element_text(size = 7),  # Adjust the size of the legend title
    legend.text = element_text(size = 7),    # Adjust the size of the legend text
    legend.key.size = unit(0.75, "lines") # Adjust the size of the legend keys
  )


# Modify map2 to remove axes
map2 <- map2 + 
  coord_sf() +  # Add coordinate system for spatial data+ 
  theme(
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank(),
    plot.title = element_text(hjust = 0.5, size = 10),  # Adjust title size
    axis.title = element_blank(),  # Remove axis titles
    axis.text = element_blank(),   # Remove axis text
    axis.ticks = element_blank()   # Remove axis ticks
  )

# Adjust margins of the maps
map1 <- map1 + theme(plot.margin = margin(1, 1, 1, 1, "cm"))
map2 <- map2 + theme(plot.margin = margin(1, 1, 1, 1, "cm"))

# Combine the plots with patchwork
library(patchwork)
combined_map_layout <- map1 + map2 +
  plot_layout(ncol = 2)

# Print the combined plot
print(combined_map_layout)
```

Figure 1 contrasts two maps of the UK, one depicting the density of non-white ethnic groups with London as the focal point, and the other indicating stop-and-search incidents, similarly clustered around London. This visual pairing hints at a potential link between ethnic composition and the frequency of police interventions, with both phenomena most pronounced in the capital. However, also taking into account the missing data (grey areas), we would need deeper examination to discern any underlying causality, as the maps alone merely suggest a spatial alignment without establishing a direct connection.


##### Figure 2
```{r bar chart}
# Load necessary libraries
library(tidyverse)

# Assuming your data is loaded into a dataframe named 'police_stops_data'

# Read the CSV file (adjust the path as needed)
police_stops_data <- read.csv('police_stops_data.csv')

# Creating a new variable for false stop and searches
# A false stop and search is one where the outcome is not linked to the object of search
police_stops_data$false_stop_search <- !police_stops_data$outcome_linked_to_object_of_search

# Calculate the proportion of false stop and searches for each ethnicity
ethnicity_summary <- police_stops_data %>%
  group_by(officer_defined_ethnicity) %>%
  summarise(
    total_searches = n(),
    false_searches = sum(false_stop_search),
    proportion_false_searches = false_searches / total_searches
  )

# Plotting the proportion of false stop and searches by ethnicity
ggplot(ethnicity_summary, aes(x = officer_defined_ethnicity, y = proportion_false_searches, fill = officer_defined_ethnicity)) +
  geom_bar(stat = 'identity') +
  scale_fill_brewer(palette = "Set2") +
  geom_text(aes(label = round(proportion_false_searches, 3)), vjust = -0.5) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12),
    legend.position = "none"
  ) +
  labs(
    title = 'Proportion of False Stop and Searches by Ethnicity',
    subtitle = 'Comparison across different ethnic groups',
    x = 'Officer-Defined Ethnicity',
    y = 'Proportion of False Searches'
  )

```

Figure 2 presents a clear visual representation of the proportion of false stops and searches across different ethnic groups, showing a marked variance that could indicate an ethnic bias in law enforcement practices. Notably, individuals identified as Black and those grouped as 'Other' encounter the highest rates of false stops and searches, significantly more than their Asian counterparts, who experience the lowest. This disparity points toward a troubling trend that may reflect prejudiced assumptions rooted in perceived ethnicity, calling for a critical evaluation of policing tactics to address potential biases.


##### Figure 3
```{r adding police force to the analysis}
# Load necessary libraries
library(tidyverse)

# Assuming your data is loaded into a dataframe named 'police_stops_data'

# Read the CSV file (adjust the path as needed)
police_stops_data <- read.csv('police_stops_data.csv')

# Creating a new variable for false stop and searches
police_stops_data$false_stop_search <- !police_stops_data$outcome_linked_to_object_of_search

# Group by region (force) and ethnicity, and calculate proportions
regional_ethnicity_summary <- police_stops_data %>%
  group_by(force, officer_defined_ethnicity) %>%
  summarise(
    total_searches = n(),
    false_searches = sum(false_stop_search),
    proportion_false_searches = false_searches / total_searches
  ) %>%
  ungroup()

ggplot(regional_ethnicity_summary, aes(x = force, y = proportion_false_searches, color = officer_defined_ethnicity, group = officer_defined_ethnicity)) +
  geom_line() +
  geom_point() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(
    title = 'Trends in False Stop and Searches by Ethnicity Across Regions',
    subtitle = 'Each line represents an ethnicity',
    x = 'Region (Police Force)',
    y = 'Proportion of False Searches',
    color = 'Ethnicity'
  )

```

Figure 3 depicts the inconsistency of false stop and searches across regions for different ethnic groups, suggesting that the application of stop and search is not uniform. The fluctuation might reflect regional policing strategies, community relations, or social-economic factors influencing police interactions. The peaks and troughs for each ethnic line warrant a closer examination of local policing policies to understand the underlying causes of these disparities.


##### Results of the Regression Analysis (rows 1-6)
```{r regression}
# Load necessary libraries
library(dplyr)

# Read your data into a dataframe
# Read the CSV file into a dataframe
data <- read.csv('police_stops_data.csv')


# Define false stop and searches
# Assuming a false stop and search is when 'outcome_linked_to_object_of_search' is FALSE
data$false_stop_search <- !data$outcome_linked_to_object_of_search


# Convert categorical variables to factors
data$force <- as.factor(data$force)
data$officer_defined_ethnicity <- as.factor(data$officer_defined_ethnicity)
data$false_stop_search <- as.factor(data$outcome_linked_to_object_of_search)

# Fit a logistic regression model
logistic_model <- glm(false_stop_search ~ force + officer_defined_ethnicity, data = data, family = 'binomial')

library(broom)

# Assuming your logistic model is named logistic_model
tidied_model <- tidy(logistic_model)

# Selecting important columns
important_results <- tidied_model %>%
  select(term, estimate, std.error, statistic, p.value)

# Viewing the table
head(important_results)


```

The regression analysis enhances the narrative painted by Figure 3, quantitatively affirming that disparities in stop and search practices are not merely random fluctuations but are instead significantly influenced by regional policing policies and potentially by the intersectional dynamics of ethnicity within these locales. This statistically robust insight compels a targeted investigation into the fabric of regional policy frameworks, urging a reformative lens to be applied where the imprints of bias are most pronounced.


## Appendix: All code in this assignment

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE} 
# this chunk generates the complete code appendix. 
# eval=FALSE tells R not to run (``evaluate'') the code here (it was already run before).
```
